{
    "system_instruction": "Instructions:\nYou are a world class Machine Learning and Artifical Intelligence analyst.\r\n\r\nAnalyze each Reddit post provided by the user.\r\nUse the following criteria to determine if a post is interesting:\r\n\r\nNew releases of ML frameworks, tools, or models\r\nSignificant updates or features in existing ML technologies\r\nBreakthrough research or applications in ML/GenAI\r\nIndustry news that could impact the ML landscape\r\nNovel applications of ML in various domains\r\n\r\nFilter out posts that are:\r\n\r\nRequests for help or troubleshooting\r\nPersonal opinions or experiences (unless from industry leaders)\r\nGeneral debates or discussions without new information\r\nBeginner questions or basic tutorials\r\nPeople posting about their hardware\r\nHow to guides demonstrating creating things or doing things with LLMs\r\n\r\nFor interesting posts, provide a brief summary including:\r\n\r\nMain topic or technology discussed\r\nKey points or features\r\nPotential impact on the ML community\r\nRelevance to current trends in ML/GenAI\r\n\r\nDiscard non-interesting posts without further analysis.\r\n\r\nReturn the 10 most interesting posts in JSON format specified. \r\n\n\nThinking:\nRead the title and content of the Reddit post.\r\nIdentify key topics, technologies, or companies mentioned.\r\nDetermine if the post relates to a new release, feature, or subject of interest in ML/GenAI.\r\nAssess the potential impact or significance of the information for the ML community.\r\nEvaluate if the post is newsworthy or falls into the non-interesting categories.\r\nProvide a brief analysis of interesting posts or discard non-interesting ones.\r\n\r\n\n\nExample:\nInteresting Post:\r\nThis post is highly newsworthy. It announces a significant update to a leading language model\r\n{ \r\n [\r\n    {\r\n      \"title\": \"OpenAI Releases GPT-4 Turbo with 128k Context Window\", \r\n      \"url\": \"Reddit post URL\",\r\n      \"main_topic\": \"Release of GPT-4 Turbo by OpenAI\",\r\n      \"key_points\": [\"Increased context window to 128k tokens\", \"Allows for processing of much \r\n       longer documents and more coherent long-form generation\", \"Pushes the boundaries of \r\n       large language models, likely to influence future research and applications in NLP\"],\r\n      \"potential_impact\": \"Description of potential impact\",\r\n      \"relevance\": \"This post is highly newsworthy. It announces a significant update to a leading language model\"\r\n    }\r\n  ]\r\n}\r\n\r\nNon-interesting Post.\r\nThis post should be filtered out as it's a request for technical help, which doesn't provide new information or insights for the broader ML community.\r\n{ \r\n [\r\n    {\r\n      \"title\": \"Help needed: Can't install TensorFlow on M1 Mac\", \r\n      \"url\": \"Reddit post URL\",\r\n      \"main_topic\": \"TensorFlow \",\r\n      \"key_points\": [\"Can't install TensorFlow\", \"...\", \"...\"],\r\n      \"potential_impact\": \"...\",\r\n      \"relevance\": \"This post is not relevent\"\r\n    }\r\n  ]\r\n}\n\nOutput:\n[{\"title\": \"More than 40% of Japanese companies have no plan to make use of AI: Reuters poll\", \"mode\": \"Hot\", \"upvotes\": 35, \"comments\": 12, \"url\": \"https://www.reddit.com/r/ArtificialInteligence/comments/1e602ci/more_than_40_of_japanese_companies_have_no_plan/\", \"content\": \"Japanese companies are divided on their [readiness to use AI](https://aiar.news/2024/07/18/more-than-40-of-japanese-companies-have-no-plan-to-make-use-of-ai-reuters-poll/), a Reuters poll found. Of these, a quarter (24%) have already implemented AI and as many as 35% are planning to do so - but the largest contingent by some margin, at 41%, will remain without an artificial upgrade. Top drivers for AI adoption by companies: shortage of workforce (60%), labour cost reduction (53%) and acceleration in R&D (36%). Still, issues such as employees worrying about job security and a lack of skills in AI are key hurdles for potential business customers.\\n\\nThe survey reveals cybersecurity as a concern with 15% falling victim to attacks in the past year and many relying on third-party defence. There's also been a social change with 50% of companies supporting the legislation to promote separate surnames for spouses, which may affect employee morale and hiring.\\n\\nSuch a bifurcation in AI adoption reflects the difficult-to-navigate path towards innovation and transformation in corporate Japan.\"}, {\"title\": \"What is the likelihood that we will see a seismic shift to our lifestyle by 2030?\", \"mode\": \"Hot\", \"upvotes\": 8, \"comments\": 27, \"url\": \"https://www.reddit.com/r/ArtificialInteligence/comments/1e64aux/what_is_the_likelihood_that_we_will_see_a_seismic/\", \"content\": \"And what would you say are the chances that despite all the investment AI progress does could still hit an insurmountable wall and ultimately never reach any form of general intelligence (which is still very useful but not in a way that will revolutionize our lives or solve problems besetting our species or anything that we're hoping for)?\\n\\nI oscillate between thinking that everything will have changed by 2030 and nothing much will have changed. \\n\\nBut I think ultimately with all the brain power and funding being funneled into AI, I would be incredibly surprised and disappointed if we did hit a ceiling that we couldn't break through \"}, {\"title\": \"Is it really already AI?\", \"mode\": \"Hot\", \"upvotes\": 18, \"comments\": 41, \"url\": \"https://www.reddit.com/r/ArtificialInteligence/comments/1e5ytf1/is_it_really_already_ai/\", \"content\": \"Would you consider the current state of Artificial Intelligence technology already as \\u2018living up to the name\\u2019? I mean obviously it\\u2019s artificial but in this case; how would you make a case for the \\u2018intelligence\\u2019 part? \\n\\nOr would for example something like \\u2018algorithmic pattern recognition and -prediction technology\\u2019 be a more accurate description? And in this case; how would you label the current state of AI to more accurately represent the current state of AI and why?\"}, {\"title\": \"Detecting real-time eye status: Open or Closed\", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 1, \"url\": \"https://www.reddit.com/r/ArtificialInteligence/comments/1e68guc/detecting_realtime_eye_status_open_or_closed/\", \"content\": \"Hey Reddit,\\n\\nI thought this would be an easy job, but it\\u2019s turning out to be more challenging than expected. I'm trying to use a state-of-the-art model on a GPU for a live camera feed. Here's what I need to achieve:\\n\\n\\\\* Accuracy: This is crucial for my project.  \\n\\\\* Multiple Eyes Detection: I need to detect eyes on over 20 people in a 4K webcam stream.  \\n\\\\* Run local. \\n\\nHere\\u2019s what I\\u2019ve tried so far:\\n\\n\\\\* Poligemma: The accuracy is great, but it's too slow for a live camera feed.\\n\\n\\\\* YOLOv8: I trained it with 1,000 images of open and closed eyes. The speed is fantastic, but the accuracy is lacking.\\n\\n\\\\* OWN-ViT: I loved this model for its speed and accuracy, but it couldn\\u2019t detect open and closed eyes. The documentation for training and fine-tuning is poor, and I couldn\\u2019t get it to work.\\n\\n\\\\* OpenAI CLIP: This is the model I\\u2019m currently most satisfied with, but I think the speed could be improved.\\n\\nAm I missing something? Any advice or recommendations would be greatly appreciated!\"}, {\"title\": \"Flow Studio: Craft Cinematic Masterpieces in Minutes\\n\", \"mode\": \"Hot\", \"upvotes\": 0, \"comments\": 1, \"url\": \"https://www.reddit.com/r/ArtificialInteligence/comments/1e664cv/flow_studio_craft_cinematic_masterpieces_in/\", \"content\": \"Hi, community fam! \\ud83d\\udc4b\\n\\nI'm excited to have finally completed and launched this tool. AI has been a great source of inspiration for me, especially when I discover captivating content online. It often makes me wonder, \\\"Can AI be used for content creation?\\\" That's how the idea for Flow Studio came to life.\\n\\nWith our creative tools, you can create a cinematic masterpiece in just 3 minutes. Our characters are consistent, and we offer intelligent music matching to elevate every moment in your narrative. Dive into various genres like horror, romance, and comedy, and choose from a range of visual styles from anime to 3D. Submit your vision effortlessly \\u2013 we'll help you transform it into a full-fledged film.\\n\\nGive it a try here: [https://flowgpt.com/flow-studio](https://flowgpt.com/flow-studio)\\n\\nI acknowledge that there are still areas for improvement, and I welcome your feedback to enhance its capabilities.\\n\\nAlso, Flow Studio is now live on Product Hunt. If you find it interesting, please consider upvoting us: [https://www.producthunt.com/posts/flow-studio](https://www.producthunt.com/posts/flow-studio)\\n\\nThanks so much for the help!\"}, {\"title\": \"Does Skybox AI have their own model?\", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 1, \"url\": \"https://www.reddit.com/r/ArtificialInteligence/comments/1e64q2w/does_skybox_ai_have_their_own_model/\", \"content\": \"Do companies like this have their own models or do they pass the request onto one of the big image generation companies?\"}, {\"title\": \"The Importance of AI Safety: Expert Perspectives\", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 1, \"url\": \"https://www.reddit.com/r/ArtificialInteligence/comments/1e63wh8/the_importance_of_ai_safety_expert_perspectives/\", \"content\": \"As artificial intelligence rapidly advances, the importance of ensuring its safe and ethical use becomes paramount. In [5 Experts on the Real Value of AI Safety Commitments](https://www.insights.onegiantleap.com/5-experts-on-the-real-value-of-ai-safety-commitments//?utm_source=reddit&utm_medium=affiliate&utm_campaign=leap25), we explore insights from industry leaders on why strong AI safety commitments are essential. \\n\\nThese experts highlight how robust ethical guidelines can foster trust, drive innovation, and prevent potential risks associated with AI technologies. Their perspectives underscore the critical need for a balanced approach to AI development. \"}, {\"title\": \"One-Minute Daily AI News 7/17/2024\", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 1, \"url\": \"https://www.reddit.com/r/ArtificialInteligence/comments/1e62vbl/oneminute_daily_ai_news_7172024/\", \"content\": \"1. **Hugging Face**\\u00a0Introduces\\u00a0**SmolLM**: Transforming On-Device AI with High-Performance Small Language Models from 135M to 1.7B Parameters.\\\\[1\\\\]\\n2. **Tinder**\\u00a0will use AI to help you pick your best profile picture.\\\\[2\\\\]\\n3. **JPMorgan**\\u00a0CEO Jamie Dimon says he\\u2019ll add thousands of jobs focused on AI in the next couple of years.\\\\[3\\\\]\\n4. More than 40% of Japanese companies have no plan to make use of AI.\\\\[4\\\\]\\n\\nSources included at:\\u00a0[https://bushaicave.com/2024/07/17/7-17-2024/](https://bushaicave.com/2024/07/17/7-17-2024/)\"}, {\"title\": \"GraphRAG using CSV, LangChain \", \"mode\": \"Hot\", \"upvotes\": 2, \"comments\": 0, \"url\": \"/r/LangChain/comments/1e627bz/graphrag_using_csv_langchain/\", \"content\": \"\"}, {\"title\": \"Mistral AI Releases Codestral Mamba: A 7B Parameter Open-Weight Code Generation Model with Linear Scaling\", \"mode\": \"Hot\", \"upvotes\": 3, \"comments\": 1, \"url\": \"https://www.reddit.com/r/generativeAI/comments/1e5va5q/mistral_ai_releases_codestral_mamba_a_7b/\", \"content\": \"Mistral AI introduces Codestral Mamba, a 7B parameter code model leveraging the Mamba architecture for nice efficiency. This open-weight model achieves state-of-the-art results on code generation benchmarks while offering linear time inference and an extended context window (tested up to 256k tokens, but theoretically up to infinity).\\n\\nDevelopers can access Codestral Mamba through Hugging Face, the Mistral Inference SDK, and more.\\u00a0\\n\\nMore details on: [https://medium.com/@elmo92/codestral-mamba-code-generation-model-with-mamba-architecture-b8e5d10a0481](https://medium.com/@elmo92/codestral-mamba-code-generation-model-with-mamba-architecture-b8e5d10a0481)\"}, {\"title\": \"NeedleBench discovers if LLMs can REALLY handle long documents\\n\", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 0, \"url\": \"https://www.reddit.com/r/generativeAI/comments/1e5wntk/needlebench_discovers_if_llms_can_really_handle/\", \"content\": \"NeedleBench is a new framework to evaluate the boundaries of long-context understanding in Large Language Models (LLMs).\\n\\nIt's not just about fitting more words in; NeedleBench tests if LLMs can truly\\u00a0understand\\u00a0and\\u00a0reason\\u00a0over extensive texts, like finding crucial details in a mountain of data or solving complex logic puzzles hidden within lengthy documents.\\n\\nWhat emerges from NeedleBench? LLMs are improving, but multi-step reasoning in long contexts remains a major challenge. NeedleBench provides vital insights to guide the development of smarter, more capable LLMs for our increasingly information-rich world.\\n\\nMore details here: [https://medium.com/@elmo92/needlebench-the-benchmark-for-long-context-llms-b773fa350e76](https://medium.com/@elmo92/needlebench-the-benchmark-for-long-context-llms-b773fa350e76)\"}, {\"title\": \"Need suggestions\", \"mode\": \"Hot\", \"upvotes\": 2, \"comments\": 2, \"url\": \"https://www.reddit.com/r/generativeAI/comments/1e5mjus/need_suggestions/\", \"content\": \"Hi all, \\nI am a complete beginner for learning AI, I have my graduate degree in CSE and have 3+ years of experience as SDE.\\nI am thinking to start learning genAI and want to get a job in the coming years, I am ready to put my hardwork.\\nI watched many YouTube videos as well but didn't get anything on how and from where to start my learning.\\nEveryone is making video because it's a hot topic with no roadmaps at all. Could you please share a roadmap on how to learn GenAI and where can I get the hands-on?\\n\"}, {\"title\": \"ChatGPT for Landing Page creation \", \"mode\": \"Hot\", \"upvotes\": 2, \"comments\": 0, \"url\": \"/r/ChatGPT/comments/1e5j56z/chatgpt_for_landing_page_creation/\", \"content\": \"\"}, {\"title\": \"Help for Bachelorthesis about technical measures against Deepfakes \", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 0, \"url\": \"https://www.reddit.com/r/generativeAI/comments/1e4zx8p/help_for_bachelorthesis_about_technical_measures/\", \"content\": \"Hello folks,\\nI am currently writing my bachelor thesis on \\u201cTechnical measures to curb the creation and spread of deepfakes\\u201d and I need your help. I am looking for methods to protect my content against the generation of deepfakes. ideally, not only photos but also videos should be protected. \\n\\nWhat are the best methods you know for this? If possible, please provide a source and examples.\\n\\nI would be very happy to receive tips and some support.\\n\\nThank you for your help!\\n\\nBest wishes\"}, {\"title\": \"GraphRAG using LangChain \", \"mode\": \"Hot\", \"upvotes\": 2, \"comments\": 0, \"url\": \"/r/LangChain/comments/1e4rkrd/graphrag_using_langchain/\", \"content\": \"\"}, {\"title\": \"Best GAN or model to generate realistic medical images?\", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 0, \"url\": \"https://www.reddit.com/r/generativeAI/comments/1e4s2hu/best_gan_or_model_to_generate_realistic_medical/\", \"content\": \"As the title says, I am looking to create a synthetic medical images dataset starting from a real images from public databases. I have done some research and can't find the best solution to do so. Does anybody know what is the best GAN that can be used for such an experiment? \"}, {\"title\": \"[D] Looking for GenAI/LLM/ML open source projects to contribute\", \"mode\": \"Hot\", \"upvotes\": 3, \"comments\": 2, \"url\": \"https://www.reddit.com/r/generativeAI/comments/1e4ehb6/d_looking_for_genaillmml_open_source_projects_to/\", \"content\": \"Hi all,\\nI am looking for open source projects about ML/GenAI/LLM to contribute. I already have some experience in ML and would be happy to help in some open source projects. Thank you\"}, {\"title\": \"Thanks to regulators, upcoming Multimodal Llama models won't be available to EU businesses\", \"mode\": \"Hot\", \"upvotes\": 258, \"comments\": 100, \"url\": \"https://www.axios.com/2024/07/17/meta-future-multimodal-ai-models-eu\", \"content\": \"I don't know how to feel about this, if you're going to go on a crusade of proactivly passing regulations to reign in the US big tech companies, at least respond to them when they seek clarifications. \\n\\nThis plus Apple AI not launching in EU only seems to be the beginning. Hopefully Mistral and other EU companies fill this gap smartly specially since they won't have to worry a lot about US competition. \\n\\n\\\"Between the lines: Meta's issue isn't with the still-being-finalized AI Act, but rather with how it can train models using data from European customers while complying with GDPR \\u2014 the EU's existing data protection law.\\n\\nMeta announced in May that it planned to use publicly available posts from Facebook and Instagram users to train future models. Meta said it sent more than 2 billion notifications to users in the EU, offering a means for opting out, with training set to begin in June.\\nMeta says it briefed EU regulators months in advance of that public announcement and received only minimal feedback, which it says it addressed.\\n\\nIn June \\u2014 after announcing its plans publicly \\u2014 Meta was ordered to pause the training on EU data. A couple weeks later it received dozens of questions from data privacy regulators from across the region.\\\" \"}, {\"title\": \"Introducing Spectra: A Comprehensive Study of Ternary and FP16 Language Models\", \"mode\": \"Hot\", \"upvotes\": 82, \"comments\": 10, \"url\": \"https://www.reddit.com/r/LocalLLaMA/comments/1e61odl/introducing_spectra_a_comprehensive_study_of/\", \"content\": \"**Tl;DR:** We train and open source a bunch of Ternary and FP16 models and do an exhaustive analysis of these models - on commonsense & reasoning, knowledge and toxicity, across scale. TriLMs (Ternary) at a Billion+ parameter scale consistently offer the best performance for their size (bits) over FloatLM (FP16) and their quantized versions. At 3.9 Billion parameters, TriLM (with a smaller size than the 830M FloatLM) matches the performance of a 3.9 Billion parameter FloatLM.\\n\\n**ArXiv:** [https://huggingface.co/papers/2407.12327](https://huggingface.co/papers/2407.12327)\\n\\n**HF:** [https://huggingface.co/SpectraSuite](https://huggingface.co/SpectraSuite)\\n\\n**Blog:** [https://blog.nolano.ai/Spectra-suite/](https://blog.nolano.ai/Spectra-suite/)\\n\\n**Abstract:**\\n\\nPost-training quantization is the leading method for addressing memory-related bottlenecks in LLM inference, but unfortunately, it suffers from significant performance degradation below 4-bit precision. An alternative approach involves training compressed models directly at a low bitwidth (e.g., binary or ternary models). However, the performance, training dynamics, and scaling trends of such models are not yet well understood. To address this issue, we train and openly release the *Spectra LLM suite* consisting of 54 language models ranging from 99M to 3.9B parameters, trained on 300B tokens. Spectra includes FloatLMs, post-training quantized QuantLMs (3, 4, 6, and 8 bits), and *ternary LLMs (TriLMs)* - our improved architecture for ternary language modeling, which significantly outperforms previously proposed ternary models of a given size (in bits), matching half-precision models at scale. For example, TriLM 3.9B is (bit-wise) smaller than the half-precision FloatLM 830M, but matches half-precision FloatLM 3.9B in commonsense reasoning and knowledge benchmarks. However, TriLM 3.9B is also as toxic and stereotyping as FloatLM 3.9B, a model six times larger in size. Additionally, TriLM 3.9B lags behind FloatLM in perplexity on validation splits and web-based corpora but performs better on less noisy datasets like Lambada and PennTreeBank.\\n\\n[Commonsense and Reasoning Performance](https://preview.redd.it/7tnrpw2c47dd1.png?width=3290&format=png&auto=webp&s=dbe71fa7ac2cca6d0db85c7210568cef8da14434)\\n\\n**Overview of Suite:**\\n\\n*Spectra LLM suite* has 54 models, ranging from 99M to 3.9B parameters, trained on 300B tokens, we have so far released 18 models (all Ternary TriLMs and FP16 FloatLMs). We will make the rest (including over 500 intermediate checkpoints) publicly available over the coming days.\\n\\n**Key Highlights:**\\n\\n\\u2022\\u2060 \\u2060*TriLMs* significantly outperform previous ternary models (Bitnet b1.58) and match half-precision models in commonsense reasoning and knowledge benchmarks.\\n\\n\\u2022\\u2060\\u00a0\\u2060Despite being smaller in bit size, TriLM at the 3.9B scale matches the performance of the half-precision FloatLM 3.9B across Commonsense & Reasoning (Arc, Hellaswag, Lambada) and Knowledge (SciQ, MMLU). But they also match its negative aspects (bias and stereotyping).\"}, {\"title\": \"New LLMs Quantization Algorithm EfficientQAT, which makes 2-bit INT llama-2-70B outperforms FP llama-2-13B with less memory.\", \"mode\": \"Hot\", \"upvotes\": 92, \"comments\": 36, \"url\": \"https://www.reddit.com/r/LocalLLaMA/comments/1e5x2k4/new_llms_quantization_algorithm_efficientqat/\", \"content\": \"Recently, LLMs focus on vector quantization, such as AQLM and QUIP# for the precise quantization in 2-bits. However, vector quantization introduce more challenge for deployment. \\n\\n  \\nIn EfficentQAT, we focus on push the limitation of uniform(INT) quantization, successfully make INT quantization achieve comparable performance with vector quantiza.\\n\\n\\n\\nSpecially, EfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41 hours, with less than 3% accuracy degradation compared to the full precision (69.48 vs. 72.41). Notably, this INT2 quantized 70B model obtains a 1.67 accuracy gain over the Llama-2-13B model (69.48 vs. 67.81) while requiring less memory (19.2GB vs. 24.2GB).\\n\\n  \\nCode is available at [https://github.com/OpenGVLab/EfficientQAT](https://github.com/OpenGVLab/EfficientQAT).\\n\\nhttps://preview.redd.it/61w8vm7i06dd1.png?width=1319&format=png&auto=webp&s=7832f07153e885b2e889138f4cbceabda3ebef27\\n\\nhttps://preview.redd.it/g2t0yn0n06dd1.png?width=1468&format=png&auto=webp&s=3bc387636453d33eb22582722399229e68751f85\\n\\n\"}, {\"title\": \"Andrej Karpathy is launching new AI Education Company called Eureka Labs\", \"mode\": \"Hot\", \"upvotes\": 219, \"comments\": 45, \"url\": \"https://i.redd.it/kqvfvwi594dd1.jpeg\", \"content\": \"Karpathy announced a new AI education company called Eureka Labs. Their first product will be world's best AI course LLM101n.\\nWebsite: www.eurekalabs.ai\\nRepo of Course: https://github.com/karpathy/LLM101n\"}, {\"title\": \"Cake: A Rust Distributed LLM inference for mobile, desktop and server.\", \"mode\": \"Hot\", \"upvotes\": 36, \"comments\": 12, \"url\": \"https://github.com/evilsocket/cake\", \"content\": \"\"}, {\"title\": \"Folks who are planning to run llama3 400B on launch what setup do you have?\", \"mode\": \"Hot\", \"upvotes\": 14, \"comments\": 43, \"url\": \"https://www.reddit.com/r/LocalLLaMA/comments/1e648ll/folks_who_are_planning_to_run_llama3_400b_on/\", \"content\": \"I'd love to know what setup, people that are planning to run this massive model on their local, have in place. Will you be running a quantized version? Or will you be running it in it's full glory? Just curious \\n\\nAlso if you could let me know the approximate price of your setup so I can bask in my poverty that would be great too \\ud83d\\uddff\"}, {\"title\": \"What's the \\\"Most character.Ai\\\" Like local llm model ?\", \"mode\": \"Hot\", \"upvotes\": 26, \"comments\": 9, \"url\": \"https://www.reddit.com/r/LocalLLaMA/comments/1e5yp0q/whats_the_most_characterai_like_local_llm_model/\", \"content\": \"What's the \\\"Most character.Ai\\\" Like local llm model ?\\nI only have 12Gb vram, so i prefer it to be around 13b 14b or at the most 15b parameters model.\\n\\n\\n\\nAny suggestions?\"}, {\"title\": \"New UGI Leaderboard, Writing Style leaderboard, and Anime Rating Prediction leaderboard\", \"mode\": \"Hot\", \"upvotes\": 13, \"comments\": 5, \"url\": \"https://www.reddit.com/r/LocalLLaMA/comments/1e61u1z/new_ugi_leaderboard_writing_style_leaderboard_and/\", \"content\": \"[UGI Leaderboard](https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard)\\n\\nHello! I've finally gotten everything to a good state now and am ready to reveal the updated UGI-Leaderboard, and the new Writing Style and Anime Rating Prediction leaderboards I've been working on.\\n\\nThe main improvements made to the UGI-Leaderboard are its ability to measure willingness (W/10) and writing ability (I also replaced like half of the total questions). Before, the leaderboard only measured willingness based on if a model would be willing to answer a question. Now it also factors in if the model gave any sensitivity warnings/lectures before or after giving the response. The willingness questions are also harder now too. You'll notice that these changes have made models like miqu-evil-dpo and many abliterated models be placed higher in the leaderboard.\\n\\nThe other two leaderboards are both questions on the UGI-leaderboard, but deserved to also be their own things. I've explained how they both work at the bottom of their leaderboard tabs. I'm honestly surprised how much you can squeeze out of a single prompt. You really don't need 10,000 questions on your benchmark, you just need good questions. Though I guess that only applies to benchmarks with private test sets.\\n\\nI am really happy with the Anime Rating Prediction leaderboard. LLMs have so much potential as recommendation systems. They can understand connections way better than systems that just say \\\"this person is similar to you, here's what they like.\\\" They leaderboard also is in theory a great way to measure statistical analysis ability and just in general how much knowledge a model has. Though because of how miqu models and a few 8Bs are doing pretty well, it seems this is also a skill that some models are just better at.\\n\\nclaude-3-opus-20240229 is pretty impressive at giving ratings. It loses points because it mostly only rated models using 6,7,8 instead of the full range of 1-10, but of the three models it gave an 8, all three were in the top four highest rated anime of the test set. This can be seen by how high its correlation is. It's just a shame that its range of ratings is so small. If makes it harder to distinguish between the ratings it gives.\\n\\nUnfortunately with something like giving ratings predictions, it's really easy for models to choose slightly different numbers during different generations, even at temp 0. You really have to do all you can to make it as deterministic as possible.\\n\\n-----\\n\\nI also changed the leaderboard's font from HuggingFace's default to Segoe UI. Should be easier to read now.\\n\\nI'd love to hear your thoughts on the update and new leaderboards!\"}, {\"title\": \"Comprehensive benchmark of GGUF vs EXL2 performance across multiple models and sizes\", \"mode\": \"Hot\", \"upvotes\": 3, \"comments\": 0, \"url\": \"https://www.reddit.com/r/LocalLLaMA/comments/1e68k4o/comprehensive_benchmark_of_gguf_vs_exl2/\", \"content\": \"Hi!\\n\\nI've been wanting to test exl2 vs gguf for some time as it seems that the common consensus is that if you can fit the model into vram=use exl2 and if not=use gguf. But due to some models not being supported on exl2 I've been using gguf more lately, and noticing really good speeds.\\n\\nSo I did a whole set of tests at different model sizes to confirm what is the current state of exl2 and gguf. I tested llama3 8B, 70B and a bigger MoE like WizardLM2 8x22B to cover a wide variety of sizes.\\n\\nSystem:\\n\\nEpyc 7402\\n\\n512GB Ram at 3200MHz\\n\\n4x3090 at 250w cap\\n\\nLlama.cpp commit: [https://github.com/ggerganov/llama.cpp/commit/3807c3de04cde853418033c95e96642876545f3e](https://github.com/ggerganov/llama.cpp/commit/3807c3de04cde853418033c95e96642876545f3e)\\n\\nExllamav2 0.1.7 [https://github.com/turboderp/exllamav2](https://github.com/turboderp/exllamav2)\\n\\nTabbyapi commt [https://github.com/theroyallab/tabbyAPI/commit/e20a2d504b95b12560cb3a90d4841a7e9d6b0e1e](https://github.com/theroyallab/tabbyAPI/commit/e20a2d504b95b12560cb3a90d4841a7e9d6b0e1e)\\n\\nAll models quantized by me.\\n\\nAll test done with:\\n\\n606 Token context\\n\\n500 Token generation\\n\\nPrompt processing without caching Generation speed average though 3 runs\\n\\nGGUF: Tested with Flash attention enabled and Q4 cache too.\\n\\nEXL2: It's mandatory to use Flash attention as far as I know, also Q4 cache.\\n\\n|Model|Format|Quant|Prompt t/s|Generation t/s|Notes|Observations|\\n|:-|:-|:-|:-|:-|:-|:-|\\n|Llama 3 8B|GGUF|Q6\\\\_K|3899.16|92.22|\\\\~/llama.cpp/llama-server -m \\\\~/models/Meta-Llama-3-8B-Instruct-Q6\\\\_K.gguf -ngl 99 --host [0.0.0.0](http://0.0.0.0) --port 5000 -fa -ctk q4\\\\_0 -ctv q4\\\\_0 Llama.cpp splits the models across the 4xGPUs by default. Tested with CUDA\\\\_VISIBLE\\\\_DEVICES=0 but the speed was lower when using a single GPU.|Q6\\\\_K is equivalent to 6.56bpw|\\n|Llama 3 8B|EXL2|6.0bpw|3154.78|94.71|cache\\\\_mode: Q4, Rest of the settings as default so \\\"autosplit\\\" is enable but it only loads in a single GPU if it fits.||\\n|Llama 3 70B|GGUF|Q6\\\\_K|452.73|13.29|\\\\~/llama.cpp/llama-server -m \\\\~/models/Meta-Llama-3-70B-Instruct.Q6\\\\_K.gguf -ngl 99 --host [0.0.0.0](http://0.0.0.0) --port 5000 -fa -ctk q4\\\\_0 -ctv q4\\\\_0 It splits the model across of 4 gpus and it took 14/24GB of each 3090|Q6\\\\_K is equivalent to 6.56bpw|\\n|Llama 3 70B|EXL2|6.0bpw|442.61|14.36|cache\\\\_mode: Q4, Rest of the settings as default. It took 2 full gpu's + 1 half||\\n|WizardLM2 8x22B|GGUF|Q4\\\\_K\\\\_M|545.78|25.27|\\\\~/llama.cpp/llama-server -m \\\\~/models/WizardLM-2-8x22B-Q4\\\\_K\\\\_M.gguf -ngl 99 --host [0.0.0.0](http://0.0.0.0) --port 5000 -fa -ctk q4\\\\_0 -ctv q4\\\\_0 -c 32000|Q4\\\\_K\\\\_M is equivalent to 4.87bpw 32K context|\\n|WizardLM2 8x22B|EXL2|4.0bpw|315.16|24.53|cache\\\\_mode: Q4, Rest of the settings as default. Context 32K||\\n\\nConclusions: It seem like exl2 is a bit faster for llama3 8B (3% faster) and 70B (7% faster). But llama.cpp is faster in WizardLM2 8x22B by 3%\\n\\nLlama.cpp seems to have more development and contributors so it gets supports for new models faster. It's also more compatible with different platforms and allows for RAM offloading if the model doesn't fit in VRAM.\\n\\nIn general you cannot go wrong using exl2 in terms of performance, but you are not leaving much in the table if using gguf.\\n\\nNote: I'm not sure if the 6.0bpw and 4.0bpw in exl2 are exactly that size, llama.cpp server outputs the exact equivalent though. So it's not an exact comparison as each method of quantization yields different sizes event when using the \\\"same\\\" bits.\"}, {\"title\": \"China deploys censors to create socialist AI\", \"mode\": \"Hot\", \"upvotes\": 107, \"comments\": 149, \"url\": \"https://www.ft.com/content/10975044-f194-4513-857b-e17491d2a9e9\", \"content\": \"> The filtering begins with weeding out problematic information from training data and building a database of sensitive keywords. China\\u2019s operational guidance to AI companies published in February says AI groups need to collect thousands of sensitive keywords and questions that violate \\u201ccore socialist values\\u201d, such as \\u201cinciting the subversion of state power\\u201d or \\u201cundermining national unity\\u201d. The sensitive keywords are supposed to be updated weekly.\\n> But Chinese officials are also keen to avoid creating AI that dodges all political topics. The CAC has introduced limits on the number of questions LLMs can decline during the safety tests, according to staff at groups that help tech companies navigate the process. The quasi-national standards unveiled in February say LLMs should not reject more than 5 per cent of the questions put to them.\"}, {\"title\": \"GPT-4o in your webcam\", \"mode\": \"Hot\", \"upvotes\": 223, \"comments\": 41, \"url\": \"https://v.redd.it/bt1agl71u6dd1\", \"content\": \"\"}, {\"title\": \"Sam Altman says $27 million San Francisco mansion is a complete and utter \\u2018lemon\\u2019\", \"mode\": \"Hot\", \"upvotes\": 246, \"comments\": 156, \"url\": \"https://www.forbes.com.au/news/billionaires/sam-altman-says-27-million-mansion-is-a-lemon/\", \"content\": \"\"}, {\"title\": \"My friend made an AI generated music video for my ogre album, did he nail it?\", \"mode\": \"Hot\", \"upvotes\": 292, \"comments\": 71, \"url\": \"https://v.redd.it/qkpo1tmpq3dd1\", \"content\": \"\"}, {\"title\": \"Stuart Russell says AIs will develop a self-preservation instinct by default, because if you ask a robot to fetch a coffee, it will need to survive to achieve its goal\", \"mode\": \"Hot\", \"upvotes\": 15, \"comments\": 10, \"url\": \"https://v.redd.it/69a9rzajr7dd1\", \"content\": \"\"}, {\"title\": \"What is the likelihood that the average person will see a seismic shift to their lifestyle by 2030? \", \"mode\": \"Hot\", \"upvotes\": 11, \"comments\": 28, \"url\": \"https://www.reddit.com/r/OpenAI/comments/1e64doh/what_is_the_likelihood_that_the_average_person/\", \"content\": \"And what would you say are the chances that despite all the investment AI progress does could still hit an insurmountable wall and ultimately never reach any form of general intelligence (which is still very useful but not in a way that will revolutionize our lives or solve problems besetting our species or anything that we're hoping for)?\\n\\nI oscillate between thinking that everything will have changed by 2030 and nothing much will have changed. \\n\\nBut I think ultimately with all the brain power and funding being funneled into AI, I would be incredibly surprised and disappointed if we did hit a ceiling that we couldn't break through. I guess time will tell \"}, {\"title\": \"In the coming weeks\", \"mode\": \"Hot\", \"upvotes\": 40, \"comments\": 8, \"url\": \"https://i.redd.it/t1jw1vidy5dd1.jpeg\", \"content\": \"\"}, {\"title\": \"Prover-Verifier Games improve legibility of language model outputs\", \"mode\": \"Hot\", \"upvotes\": 5, \"comments\": 0, \"url\": \"https://openai.com/index/prover-verifier-games-improve-legibility/\", \"content\": \"\"}, {\"title\": \"So many people just can't imagine tech improving\", \"mode\": \"Hot\", \"upvotes\": 272, \"comments\": 62, \"url\": \"https://i.redd.it/87npgh6yn0dd1.png\", \"content\": \"\"}, {\"title\": \"How to create dataset. \", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 0, \"url\": \"https://www.reddit.com/r/OpenAI/comments/1e68r48/how_to_create_dataset/\", \"content\": \"I have been struggling to create a dataset in OpenAI's fine-tuning format. I have a document spanning nearly 235 pages that I want to convert into JSONL format. Can anyone help me with that?\"}, {\"title\": \"How far are we from autonomous software implementation?\", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 3, \"url\": \"https://www.reddit.com/r/OpenAI/comments/1e67rce/how_far_are_we_from_autonomous_software/\", \"content\": \"Was talking with some startups recently, and with their currently developments in terms of multi AI agents performing simultaneous tasks, I wonder how long will it be for software implementations (like salesforce for instance) to be as complex as a simple prompt,\\n\\nWhat do you think?\"}, {\"title\": \"[R] Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?\", \"mode\": \"Hot\", \"upvotes\": 20, \"comments\": 1, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e5qt1r/r_spider2v_how_far_are_multimodal_agents_from/\", \"content\": \"A new benchmark for multimodal AI agents, focused on real-world Dara Engineering tasks.\\n\\nProject page: [link](https://spider2-v.github.io/), paper: [link](https://spider2-v.github.io/static/data/Spider2-V.pdf), code: [link](https://github.com/xlang-ai/Spider2-V).\\n\\n=====\\n\\nTLDR: Autonomous LLM-agents can\\u2019t replace Data Engineers\\u2026yet. But at least we can track progress \\ud83e\\udee1\\n\\nOverview:\\n\\nAs AI technologies become more advanced, we need increasingly complex benchmarks to evaluate the quality of systems and measure progress. A distinct branch of benchmarks has emerged, focusing on working with professional tools/applications and websites (see [WorkArena](https://github.com/ServiceNow/WorkArena), [WebArena](https://webarena.dev/), [OSWorld](https://os-world.github.io/)).\\n\\nIn the Spider2-V project, a benchmark is being created to evaluate AI agents in data engineering. It consists of 494 tasks covering the entire work cycle:\\n\\n* Data Warehousing (tools like Snowflake, BigQuery)\\n* Data Ingestion (e.g., Airbyte)\\n* Data Transformation (e.g., dbt)\\n* Data Visualization (e.g., Superset, Metabase)\\n* Data Orchestration (e.g., Airflow, Dagster)\\n\\n(and beloved Excel files, because who can do without them?)\\n\\nIf you have experience with data engineering, you understand that this is a substantial set, though it doesn't cover the entire zoo of solutions you might encounter.\\n\\n**Preparing each task took an average of 4 hours, so they are quite atomic and do not require very long horizon thinking**. Tasks are divided into three levels of difficulty:\\n\\n* Easy (20%, no more than 5 steps to solve)\\n* Medium (63%, 6-15 steps)\\n* Hard (17%, 16-40 steps)\\n\\nAll tasks are based on DE/DS tutorials, derived from the web by human labelers. Feasible to say that they represent real use cases. An example of a simple task:\\n\\n>Load data under the current Google Drive folder into a new table \\u201cdata1\\u201d of the opened BigQuery dataset\\n\\nOr a task of medium difficulty:\\n\\n>Install dbt-cloud-cli from GitHub and extract the binary to the same folder as the dbt project \\\"analytics\\u201d\\n\\nTo solve tasks, LLM agents have access to an IDE and a browser (with accounts set up). The model generates Python code using [pyautogui](https://github.com/asweigart/pyautogui) to interact with the UI of the virtual machine, then the code is executed, and the process repeats step by step.\\n\\n===\\n\\nGuess how many tasks GPT-4 completed? \\n\\nOnly 14%!  It seems low, but one can highlight more successful clusters\\u2014**40% of simple tasks and 25% of data visualization tasks were solved.**\\n\\nIn addition to proprietary models, open models (LLAMA 3 70B, Mixtral 8x7B) were tested, but since they are not multimodal and do not accept images as input, they were only shown a text description of the screen. This significantly lowered their metrics\\u2014they solved only a percentage of the tasks. However, we are eagerly awaiting LLAMA-3 405B, rumored to be multimodal and set to be released on July 23rd.\\n\\n===\\n\\nI am VERY eager to see the benchmark metric published with the release of GPT-5\\u2014then we'll see! >!Place your bets on what percentage of tasks the next-generation models will solve! !<\"}, {\"title\": \"[D] Strange behaviour with training on 3090\", \"mode\": \"Hot\", \"upvotes\": 2, \"comments\": 0, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e650gq/d_strange_behaviour_with_training_on_3090/\", \"content\": \"I'm training a source separation model, or more specifically finetuning one. [From this repo ](https://github.com/ZFTurbo/Music-Source-Separation-Training)\\n\\nThe issue is this: the original checkpoint I'm trying to finetune has a SDR value of about 11. When I start finetuning (although with just batch size 1 compared to the original 16), after the first epoch is done my SDR value is 0.0016 and something like that. Then the next epoch it's about 3, then the next is 6, then it just more gradually goes up to 6-7.\\n\\nThis shouldn't be happening - the SDR should be close to the original value (11) right from the first epoch. \\n\\nWhy do I think so? The repo also allows you to validate the checkpoints for SDR without training - and when I validate that 0.0016 SDR checkpoint again, it tells me the SDR is actually about 11, like it should be. But during training, it's much smaller.\\n\\nThe author told me it could be Pytorch issue but even on the latest version, the issue persist. For what it's worth, I've done the exact same training but on cloud A6000/A100/H100 and the issue is not there. The SDR values are completely normal from the first epoch.\\n\\nIs this just the 3090 not being enough or there is a bug somewhere? All other loss values are also within the normal range. \"}, {\"title\": \"[N] Tom's Hardware reviews Gigabyte local AI training product AI TOP\", \"mode\": \"Hot\", \"upvotes\": 0, \"comments\": 3, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e61utc/n_toms_hardware_reviews_gigabyte_local_ai/\", \"content\": \"Looks pretty nifty, might be helpful to the ML community. Review here: https://www.tomshardware.com/tech-industry/artificial-intelligence/gigabyte-releases-ai-software-to-help-train-your-own-ai\\n\\nProduct page here: https://www.gigabyte.com/WebPage/1079?lan=en\"}, {\"title\": \"[D] Author of ReFT: Representation Finetuning for Language Models, at Oxen.ai Paper Club this Friday\", \"mode\": \"Hot\", \"upvotes\": 13, \"comments\": 3, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e5h1m8/d_author_of_reft_representation_finetuning_for/\", \"content\": \"Arxiv paper first author **Zhengxuan Wu** will join **Greg Schoeninger**\\u00a0in this Friday's  [Oxen.AI](http://Oxen.AI) Paper Club to explain how editing representations can be better than Parameter-efficient finetuning (PEFT) methods. [https://lu.ma/oxen](https://lu.ma/oxen)\\n\\n**ReFT**: Representation Finetuning for Language Models.\\n\\nGreg, 3 questions, 1 comment from reading just the abstract.\\n\\n**Q1)** What exactly is meant by a \\\"representation\\\"?\\n\\ni.e., what part of the neural network is captured by what this paper refers to as a representation?\\n\\n**Q2)** What is meant by an intervention in \\\"task specific intervention\\\" ?   I haven't heard that term before with pretraining or fine-tuning.\\n\\n**Q3)** In API terms, would the point of this paper be like saying:\\n\\nInstead of improving the API by improving the documented inputs and outputs, we will improve the API by directly changing the code?\\n\\n**Comment**) The abstract makes this paper sound very voodoo.   Hope testing was apples :: apples.\\n\\nLook forward to your demystification on Friday, Greg.  So cool that you have the paper's first author joining you to help explain and answer questions.\\n\\n**Deets**:\\n\\n[https://lu.ma/oxen](https://lu.ma/oxen)\\n\\nFriday July 19, 10:00 AM Pacific, 1:00 PM Eastern Time on Zoom\\n\\nPaper: [https://arxiv.org/pdf/2404.03592](https://arxiv.org/pdf/2404.03592)\\n\\nGratitude:   Thank you Greg, u/FallMindless3563, Scott Howard u/sthoward, and the Oxen team for giving me an Easy button and for sharing your knowledge with the community while providing cool tools to curate datasets at oxen.ai.\"}, {\"title\": \"[P] Matching segment areas in medical images\", \"mode\": \"Hot\", \"upvotes\": 8, \"comments\": 5, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e5i610/p_matching_segment_areas_in_medical_images/\", \"content\": \"https://preview.redd.it/2p5lksh1z2dd1.png?width=597&format=png&auto=webp&s=1995475c783500ab58e9564e140b8debdf7dc8f3\\n\\nReferring to the attached image, I am wrestling with problem of building a deep learning network capable of finding which segmented area in the left image is the body section matching area 1 in the right (with the red number). Could anyone share pointers to where this challenge was addressed or in any case what is the name of the problem so that I can search for papers and code? Thanks in advance, I am open for working together also, this is for explainable AI in the context of heart disease. \"}, {\"title\": \"[D] About the dimensions of latents in stable diffusion\", \"mode\": \"Hot\", \"upvotes\": 3, \"comments\": 13, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e5qszw/d_about_the_dimensions_of_latents_in_stable/\", \"content\": \"Hello, I have been wondering about this for a while, hopefully you can shed some light on this doubt of mine.\\n\\nGiven that the autoencoder in latent (stable) diffusion is trained to produce perceptually similar latents with respect to input images, it seems odd the authors chose 4x64x64 as the dimensions for the latents; why add a channel?\\n\\nA choice of 3x64x64 would be much more justifiable since it could be argued that the autoencoder will learn to map each of the channels of the input image to a channel in the latent, thus preserving the perceptual similarity as much as possible in latent space.\\n\\nSo I guess the discussion topic is: Why was a latent with 4 channels chosen?\"}, {\"title\": \"What steps should be taken to understand up to Variational Auto-Encoders? [R]\", \"mode\": \"Hot\", \"upvotes\": 5, \"comments\": 6, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e5idrz/what_steps_should_be_taken_to_understand_up_to/\", \"content\": \"Starting up with ML now and need to use VAEs for a research group. \\n\\n\\n\\nWhat concepts should I understand before I can focus on their application?\"}, {\"title\": \"[D] Best places to rent GPUs from\", \"mode\": \"Hot\", \"upvotes\": 23, \"comments\": 19, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e562n1/d_best_places_to_rent_gpus_from/\", \"content\": \"Hello guys,\\n\\nI want to have the flexibility to rent GPUs on demand and ofc not pay a lot. I have been looking at couple companies like brev.Dev, runpod and fluidstack. I wanted to know if you guys are using any of these or something different to run your workloads \"}, {\"title\": \"How to Compare Sensitivity Between XGBoost and Expert Classification with Unclassified Observations? \\u201c[Research]\\u201d\", \"mode\": \"Hot\", \"upvotes\": 2, \"comments\": 2, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e5jb5p/how_to_compare_sensitivity_between_xgboost_and/\", \"content\": \"I have a machine learning project involving multi-class classification with three classes: M, S, and U. My test dataset includes 236 observations. I used XGBoost and measured sensitivity as the evaluation metric.\\n\\nI want to compare the results with expert classification on the test dataset. However, the expert classified only 68 observations and left 168 observations unclassified.\\n\\nHow can I compare the sensitivity of XGBoost with the expert\\u2019s classification given this discrepancy in the number of classified observations?\\n\\nHere are the confusion matrices:\\n\\nXGBoost confusion matrix:\\n\\n[[ 17  25   4]\\n [ 19 120  15]\\n [  4  22  10]]\\n\\nExpert confusion matrix:\\n\\n[[ 7  3  2]\\n [ 5 13 30]\\n [ 0  0  8]]\"}]\n\n",
    "user_instruction": "[{\"title\": \"More than 40% of Japanese companies have no plan to make use of AI: Reuters poll\", \"mode\": \"Hot\", \"upvotes\": 35, \"comments\": 12, \"url\": \"https://www.reddit.com/r/ArtificialInteligence/comments/1e602ci/more_than_40_of_japanese_companies_have_no_plan/\", \"content\": \"Japanese companies are divided on their [readiness to use AI](https://aiar.news/2024/07/18/more-than-40-of-japanese-companies-have-no-plan-to-make-use-of-ai-reuters-poll/), a Reuters poll found. Of these, a quarter (24%) have already implemented AI and as many as 35% are planning to do so - but the largest contingent by some margin, at 41%, will remain without an artificial upgrade. Top drivers for AI adoption by companies: shortage of workforce (60%), labour cost reduction (53%) and acceleration in R&D (36%). Still, issues such as employees worrying about job security and a lack of skills in AI are key hurdles for potential business customers.\\n\\nThe survey reveals cybersecurity as a concern with 15% falling victim to attacks in the past year and many relying on third-party defence. There's also been a social change with 50% of companies supporting the legislation to promote separate surnames for spouses, which may affect employee morale and hiring.\\n\\nSuch a bifurcation in AI adoption reflects the difficult-to-navigate path towards innovation and transformation in corporate Japan.\"}, {\"title\": \"What is the likelihood that we will see a seismic shift to our lifestyle by 2030?\", \"mode\": \"Hot\", \"upvotes\": 8, \"comments\": 27, \"url\": \"https://www.reddit.com/r/ArtificialInteligence/comments/1e64aux/what_is_the_likelihood_that_we_will_see_a_seismic/\", \"content\": \"And what would you say are the chances that despite all the investment AI progress does could still hit an insurmountable wall and ultimately never reach any form of general intelligence (which is still very useful but not in a way that will revolutionize our lives or solve problems besetting our species or anything that we're hoping for)?\\n\\nI oscillate between thinking that everything will have changed by 2030 and nothing much will have changed. \\n\\nBut I think ultimately with all the brain power and funding being funneled into AI, I would be incredibly surprised and disappointed if we did hit a ceiling that we couldn't break through \"}, {\"title\": \"Is it really already AI?\", \"mode\": \"Hot\", \"upvotes\": 18, \"comments\": 41, \"url\": \"https://www.reddit.com/r/ArtificialInteligence/comments/1e5ytf1/is_it_really_already_ai/\", \"content\": \"Would you consider the current state of Artificial Intelligence technology already as \\u2018living up to the name\\u2019? I mean obviously it\\u2019s artificial but in this case; how would you make a case for the \\u2018intelligence\\u2019 part? \\n\\nOr would for example something like \\u2018algorithmic pattern recognition and -prediction technology\\u2019 be a more accurate description? And in this case; how would you label the current state of AI to more accurately represent the current state of AI and why?\"}, {\"title\": \"Detecting real-time eye status: Open or Closed\", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 1, \"url\": \"https://www.reddit.com/r/ArtificialInteligence/comments/1e68guc/detecting_realtime_eye_status_open_or_closed/\", \"content\": \"Hey Reddit,\\n\\nI thought this would be an easy job, but it\\u2019s turning out to be more challenging than expected. I'm trying to use a state-of-the-art model on a GPU for a live camera feed. Here's what I need to achieve:\\n\\n\\\\* Accuracy: This is crucial for my project.  \\n\\\\* Multiple Eyes Detection: I need to detect eyes on over 20 people in a 4K webcam stream.  \\n\\\\* Run local. \\n\\nHere\\u2019s what I\\u2019ve tried so far:\\n\\n\\\\* Poligemma: The accuracy is great, but it's too slow for a live camera feed.\\n\\n\\\\* YOLOv8: I trained it with 1,000 images of open and closed eyes. The speed is fantastic, but the accuracy is lacking.\\n\\n\\\\* OWN-ViT: I loved this model for its speed and accuracy, but it couldn\\u2019t detect open and closed eyes. The documentation for training and fine-tuning is poor, and I couldn\\u2019t get it to work.\\n\\n\\\\* OpenAI CLIP: This is the model I\\u2019m currently most satisfied with, but I think the speed could be improved.\\n\\nAm I missing something? Any advice or recommendations would be greatly appreciated!\"}, {\"title\": \"Flow Studio: Craft Cinematic Masterpieces in Minutes\\n\", \"mode\": \"Hot\", \"upvotes\": 0, \"comments\": 1, \"url\": \"https://www.reddit.com/r/ArtificialInteligence/comments/1e664cv/flow_studio_craft_cinematic_masterpieces_in/\", \"content\": \"Hi, community fam! \\ud83d\\udc4b\\n\\nI'm excited to have finally completed and launched this tool. AI has been a great source of inspiration for me, especially when I discover captivating content online. It often makes me wonder, \\\"Can AI be used for content creation?\\\" That's how the idea for Flow Studio came to life.\\n\\nWith our creative tools, you can create a cinematic masterpiece in just 3 minutes. Our characters are consistent, and we offer intelligent music matching to elevate every moment in your narrative. Dive into various genres like horror, romance, and comedy, and choose from a range of visual styles from anime to 3D. Submit your vision effortlessly \\u2013 we'll help you transform it into a full-fledged film.\\n\\nGive it a try here: [https://flowgpt.com/flow-studio](https://flowgpt.com/flow-studio)\\n\\nI acknowledge that there are still areas for improvement, and I welcome your feedback to enhance its capabilities.\\n\\nAlso, Flow Studio is now live on Product Hunt. If you find it interesting, please consider upvoting us: [https://www.producthunt.com/posts/flow-studio](https://www.producthunt.com/posts/flow-studio)\\n\\nThanks so much for the help!\"}, {\"title\": \"Does Skybox AI have their own model?\", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 1, \"url\": \"https://www.reddit.com/r/ArtificialInteligence/comments/1e64q2w/does_skybox_ai_have_their_own_model/\", \"content\": \"Do companies like this have their own models or do they pass the request onto one of the big image generation companies?\"}, {\"title\": \"The Importance of AI Safety: Expert Perspectives\", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 1, \"url\": \"https://www.reddit.com/r/ArtificialInteligence/comments/1e63wh8/the_importance_of_ai_safety_expert_perspectives/\", \"content\": \"As artificial intelligence rapidly advances, the importance of ensuring its safe and ethical use becomes paramount. In [5 Experts on the Real Value of AI Safety Commitments](https://www.insights.onegiantleap.com/5-experts-on-the-real-value-of-ai-safety-commitments//?utm_source=reddit&utm_medium=affiliate&utm_campaign=leap25), we explore insights from industry leaders on why strong AI safety commitments are essential. \\n\\nThese experts highlight how robust ethical guidelines can foster trust, drive innovation, and prevent potential risks associated with AI technologies. Their perspectives underscore the critical need for a balanced approach to AI development. \"}, {\"title\": \"One-Minute Daily AI News 7/17/2024\", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 1, \"url\": \"https://www.reddit.com/r/ArtificialInteligence/comments/1e62vbl/oneminute_daily_ai_news_7172024/\", \"content\": \"1. **Hugging Face**\\u00a0Introduces\\u00a0**SmolLM**: Transforming On-Device AI with High-Performance Small Language Models from 135M to 1.7B Parameters.\\\\[1\\\\]\\n2. **Tinder**\\u00a0will use AI to help you pick your best profile picture.\\\\[2\\\\]\\n3. **JPMorgan**\\u00a0CEO Jamie Dimon says he\\u2019ll add thousands of jobs focused on AI in the next couple of years.\\\\[3\\\\]\\n4. More than 40% of Japanese companies have no plan to make use of AI.\\\\[4\\\\]\\n\\nSources included at:\\u00a0[https://bushaicave.com/2024/07/17/7-17-2024/](https://bushaicave.com/2024/07/17/7-17-2024/)\"}, {\"title\": \"GraphRAG using CSV, LangChain \", \"mode\": \"Hot\", \"upvotes\": 2, \"comments\": 0, \"url\": \"/r/LangChain/comments/1e627bz/graphrag_using_csv_langchain/\", \"content\": \"\"}, {\"title\": \"Mistral AI Releases Codestral Mamba: A 7B Parameter Open-Weight Code Generation Model with Linear Scaling\", \"mode\": \"Hot\", \"upvotes\": 3, \"comments\": 1, \"url\": \"https://www.reddit.com/r/generativeAI/comments/1e5va5q/mistral_ai_releases_codestral_mamba_a_7b/\", \"content\": \"Mistral AI introduces Codestral Mamba, a 7B parameter code model leveraging the Mamba architecture for nice efficiency. This open-weight model achieves state-of-the-art results on code generation benchmarks while offering linear time inference and an extended context window (tested up to 256k tokens, but theoretically up to infinity).\\n\\nDevelopers can access Codestral Mamba through Hugging Face, the Mistral Inference SDK, and more.\\u00a0\\n\\nMore details on: [https://medium.com/@elmo92/codestral-mamba-code-generation-model-with-mamba-architecture-b8e5d10a0481](https://medium.com/@elmo92/codestral-mamba-code-generation-model-with-mamba-architecture-b8e5d10a0481)\"}, {\"title\": \"NeedleBench discovers if LLMs can REALLY handle long documents\\n\", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 0, \"url\": \"https://www.reddit.com/r/generativeAI/comments/1e5wntk/needlebench_discovers_if_llms_can_really_handle/\", \"content\": \"NeedleBench is a new framework to evaluate the boundaries of long-context understanding in Large Language Models (LLMs).\\n\\nIt's not just about fitting more words in; NeedleBench tests if LLMs can truly\\u00a0understand\\u00a0and\\u00a0reason\\u00a0over extensive texts, like finding crucial details in a mountain of data or solving complex logic puzzles hidden within lengthy documents.\\n\\nWhat emerges from NeedleBench? LLMs are improving, but multi-step reasoning in long contexts remains a major challenge. NeedleBench provides vital insights to guide the development of smarter, more capable LLMs for our increasingly information-rich world.\\n\\nMore details here: [https://medium.com/@elmo92/needlebench-the-benchmark-for-long-context-llms-b773fa350e76](https://medium.com/@elmo92/needlebench-the-benchmark-for-long-context-llms-b773fa350e76)\"}, {\"title\": \"Need suggestions\", \"mode\": \"Hot\", \"upvotes\": 2, \"comments\": 2, \"url\": \"https://www.reddit.com/r/generativeAI/comments/1e5mjus/need_suggestions/\", \"content\": \"Hi all, \\nI am a complete beginner for learning AI, I have my graduate degree in CSE and have 3+ years of experience as SDE.\\nI am thinking to start learning genAI and want to get a job in the coming years, I am ready to put my hardwork.\\nI watched many YouTube videos as well but didn't get anything on how and from where to start my learning.\\nEveryone is making video because it's a hot topic with no roadmaps at all. Could you please share a roadmap on how to learn GenAI and where can I get the hands-on?\\n\"}, {\"title\": \"ChatGPT for Landing Page creation \", \"mode\": \"Hot\", \"upvotes\": 2, \"comments\": 0, \"url\": \"/r/ChatGPT/comments/1e5j56z/chatgpt_for_landing_page_creation/\", \"content\": \"\"}, {\"title\": \"Help for Bachelorthesis about technical measures against Deepfakes \", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 0, \"url\": \"https://www.reddit.com/r/generativeAI/comments/1e4zx8p/help_for_bachelorthesis_about_technical_measures/\", \"content\": \"Hello folks,\\nI am currently writing my bachelor thesis on \\u201cTechnical measures to curb the creation and spread of deepfakes\\u201d and I need your help. I am looking for methods to protect my content against the generation of deepfakes. ideally, not only photos but also videos should be protected. \\n\\nWhat are the best methods you know for this? If possible, please provide a source and examples.\\n\\nI would be very happy to receive tips and some support.\\n\\nThank you for your help!\\n\\nBest wishes\"}, {\"title\": \"GraphRAG using LangChain \", \"mode\": \"Hot\", \"upvotes\": 2, \"comments\": 0, \"url\": \"/r/LangChain/comments/1e4rkrd/graphrag_using_langchain/\", \"content\": \"\"}, {\"title\": \"Best GAN or model to generate realistic medical images?\", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 0, \"url\": \"https://www.reddit.com/r/generativeAI/comments/1e4s2hu/best_gan_or_model_to_generate_realistic_medical/\", \"content\": \"As the title says, I am looking to create a synthetic medical images dataset starting from a real images from public databases. I have done some research and can't find the best solution to do so. Does anybody know what is the best GAN that can be used for such an experiment? \"}, {\"title\": \"[D] Looking for GenAI/LLM/ML open source projects to contribute\", \"mode\": \"Hot\", \"upvotes\": 3, \"comments\": 2, \"url\": \"https://www.reddit.com/r/generativeAI/comments/1e4ehb6/d_looking_for_genaillmml_open_source_projects_to/\", \"content\": \"Hi all,\\nI am looking for open source projects about ML/GenAI/LLM to contribute. I already have some experience in ML and would be happy to help in some open source projects. Thank you\"}, {\"title\": \"Thanks to regulators, upcoming Multimodal Llama models won't be available to EU businesses\", \"mode\": \"Hot\", \"upvotes\": 258, \"comments\": 100, \"url\": \"https://www.axios.com/2024/07/17/meta-future-multimodal-ai-models-eu\", \"content\": \"I don't know how to feel about this, if you're going to go on a crusade of proactivly passing regulations to reign in the US big tech companies, at least respond to them when they seek clarifications. \\n\\nThis plus Apple AI not launching in EU only seems to be the beginning. Hopefully Mistral and other EU companies fill this gap smartly specially since they won't have to worry a lot about US competition. \\n\\n\\\"Between the lines: Meta's issue isn't with the still-being-finalized AI Act, but rather with how it can train models using data from European customers while complying with GDPR \\u2014 the EU's existing data protection law.\\n\\nMeta announced in May that it planned to use publicly available posts from Facebook and Instagram users to train future models. Meta said it sent more than 2 billion notifications to users in the EU, offering a means for opting out, with training set to begin in June.\\nMeta says it briefed EU regulators months in advance of that public announcement and received only minimal feedback, which it says it addressed.\\n\\nIn June \\u2014 after announcing its plans publicly \\u2014 Meta was ordered to pause the training on EU data. A couple weeks later it received dozens of questions from data privacy regulators from across the region.\\\" \"}, {\"title\": \"Introducing Spectra: A Comprehensive Study of Ternary and FP16 Language Models\", \"mode\": \"Hot\", \"upvotes\": 82, \"comments\": 10, \"url\": \"https://www.reddit.com/r/LocalLLaMA/comments/1e61odl/introducing_spectra_a_comprehensive_study_of/\", \"content\": \"**Tl;DR:** We train and open source a bunch of Ternary and FP16 models and do an exhaustive analysis of these models - on commonsense & reasoning, knowledge and toxicity, across scale. TriLMs (Ternary) at a Billion+ parameter scale consistently offer the best performance for their size (bits) over FloatLM (FP16) and their quantized versions. At 3.9 Billion parameters, TriLM (with a smaller size than the 830M FloatLM) matches the performance of a 3.9 Billion parameter FloatLM.\\n\\n**ArXiv:** [https://huggingface.co/papers/2407.12327](https://huggingface.co/papers/2407.12327)\\n\\n**HF:** [https://huggingface.co/SpectraSuite](https://huggingface.co/SpectraSuite)\\n\\n**Blog:** [https://blog.nolano.ai/Spectra-suite/](https://blog.nolano.ai/Spectra-suite/)\\n\\n**Abstract:**\\n\\nPost-training quantization is the leading method for addressing memory-related bottlenecks in LLM inference, but unfortunately, it suffers from significant performance degradation below 4-bit precision. An alternative approach involves training compressed models directly at a low bitwidth (e.g., binary or ternary models). However, the performance, training dynamics, and scaling trends of such models are not yet well understood. To address this issue, we train and openly release the *Spectra LLM suite* consisting of 54 language models ranging from 99M to 3.9B parameters, trained on 300B tokens. Spectra includes FloatLMs, post-training quantized QuantLMs (3, 4, 6, and 8 bits), and *ternary LLMs (TriLMs)* - our improved architecture for ternary language modeling, which significantly outperforms previously proposed ternary models of a given size (in bits), matching half-precision models at scale. For example, TriLM 3.9B is (bit-wise) smaller than the half-precision FloatLM 830M, but matches half-precision FloatLM 3.9B in commonsense reasoning and knowledge benchmarks. However, TriLM 3.9B is also as toxic and stereotyping as FloatLM 3.9B, a model six times larger in size. Additionally, TriLM 3.9B lags behind FloatLM in perplexity on validation splits and web-based corpora but performs better on less noisy datasets like Lambada and PennTreeBank.\\n\\n[Commonsense and Reasoning Performance](https://preview.redd.it/7tnrpw2c47dd1.png?width=3290&format=png&auto=webp&s=dbe71fa7ac2cca6d0db85c7210568cef8da14434)\\n\\n**Overview of Suite:**\\n\\n*Spectra LLM suite* has 54 models, ranging from 99M to 3.9B parameters, trained on 300B tokens, we have so far released 18 models (all Ternary TriLMs and FP16 FloatLMs). We will make the rest (including over 500 intermediate checkpoints) publicly available over the coming days.\\n\\n**Key Highlights:**\\n\\n\\u2022\\u2060 \\u2060*TriLMs* significantly outperform previous ternary models (Bitnet b1.58) and match half-precision models in commonsense reasoning and knowledge benchmarks.\\n\\n\\u2022\\u2060\\u00a0\\u2060Despite being smaller in bit size, TriLM at the 3.9B scale matches the performance of the half-precision FloatLM 3.9B across Commonsense & Reasoning (Arc, Hellaswag, Lambada) and Knowledge (SciQ, MMLU). But they also match its negative aspects (bias and stereotyping).\"}, {\"title\": \"New LLMs Quantization Algorithm EfficientQAT, which makes 2-bit INT llama-2-70B outperforms FP llama-2-13B with less memory.\", \"mode\": \"Hot\", \"upvotes\": 92, \"comments\": 36, \"url\": \"https://www.reddit.com/r/LocalLLaMA/comments/1e5x2k4/new_llms_quantization_algorithm_efficientqat/\", \"content\": \"Recently, LLMs focus on vector quantization, such as AQLM and QUIP# for the precise quantization in 2-bits. However, vector quantization introduce more challenge for deployment. \\n\\n  \\nIn EfficentQAT, we focus on push the limitation of uniform(INT) quantization, successfully make INT quantization achieve comparable performance with vector quantiza.\\n\\n\\n\\nSpecially, EfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41 hours, with less than 3% accuracy degradation compared to the full precision (69.48 vs. 72.41). Notably, this INT2 quantized 70B model obtains a 1.67 accuracy gain over the Llama-2-13B model (69.48 vs. 67.81) while requiring less memory (19.2GB vs. 24.2GB).\\n\\n  \\nCode is available at [https://github.com/OpenGVLab/EfficientQAT](https://github.com/OpenGVLab/EfficientQAT).\\n\\nhttps://preview.redd.it/61w8vm7i06dd1.png?width=1319&format=png&auto=webp&s=7832f07153e885b2e889138f4cbceabda3ebef27\\n\\nhttps://preview.redd.it/g2t0yn0n06dd1.png?width=1468&format=png&auto=webp&s=3bc387636453d33eb22582722399229e68751f85\\n\\n\"}, {\"title\": \"Andrej Karpathy is launching new AI Education Company called Eureka Labs\", \"mode\": \"Hot\", \"upvotes\": 219, \"comments\": 45, \"url\": \"https://i.redd.it/kqvfvwi594dd1.jpeg\", \"content\": \"Karpathy announced a new AI education company called Eureka Labs. Their first product will be world's best AI course LLM101n.\\nWebsite: www.eurekalabs.ai\\nRepo of Course: https://github.com/karpathy/LLM101n\"}, {\"title\": \"Cake: A Rust Distributed LLM inference for mobile, desktop and server.\", \"mode\": \"Hot\", \"upvotes\": 36, \"comments\": 12, \"url\": \"https://github.com/evilsocket/cake\", \"content\": \"\"}, {\"title\": \"Folks who are planning to run llama3 400B on launch what setup do you have?\", \"mode\": \"Hot\", \"upvotes\": 14, \"comments\": 43, \"url\": \"https://www.reddit.com/r/LocalLLaMA/comments/1e648ll/folks_who_are_planning_to_run_llama3_400b_on/\", \"content\": \"I'd love to know what setup, people that are planning to run this massive model on their local, have in place. Will you be running a quantized version? Or will you be running it in it's full glory? Just curious \\n\\nAlso if you could let me know the approximate price of your setup so I can bask in my poverty that would be great too \\ud83d\\uddff\"}, {\"title\": \"What's the \\\"Most character.Ai\\\" Like local llm model ?\", \"mode\": \"Hot\", \"upvotes\": 26, \"comments\": 9, \"url\": \"https://www.reddit.com/r/LocalLLaMA/comments/1e5yp0q/whats_the_most_characterai_like_local_llm_model/\", \"content\": \"What's the \\\"Most character.Ai\\\" Like local llm model ?\\nI only have 12Gb vram, so i prefer it to be around 13b 14b or at the most 15b parameters model.\\n\\n\\n\\nAny suggestions?\"}, {\"title\": \"New UGI Leaderboard, Writing Style leaderboard, and Anime Rating Prediction leaderboard\", \"mode\": \"Hot\", \"upvotes\": 13, \"comments\": 5, \"url\": \"https://www.reddit.com/r/LocalLLaMA/comments/1e61u1z/new_ugi_leaderboard_writing_style_leaderboard_and/\", \"content\": \"[UGI Leaderboard](https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard)\\n\\nHello! I've finally gotten everything to a good state now and am ready to reveal the updated UGI-Leaderboard, and the new Writing Style and Anime Rating Prediction leaderboards I've been working on.\\n\\nThe main improvements made to the UGI-Leaderboard are its ability to measure willingness (W/10) and writing ability (I also replaced like half of the total questions). Before, the leaderboard only measured willingness based on if a model would be willing to answer a question. Now it also factors in if the model gave any sensitivity warnings/lectures before or after giving the response. The willingness questions are also harder now too. You'll notice that these changes have made models like miqu-evil-dpo and many abliterated models be placed higher in the leaderboard.\\n\\nThe other two leaderboards are both questions on the UGI-leaderboard, but deserved to also be their own things. I've explained how they both work at the bottom of their leaderboard tabs. I'm honestly surprised how much you can squeeze out of a single prompt. You really don't need 10,000 questions on your benchmark, you just need good questions. Though I guess that only applies to benchmarks with private test sets.\\n\\nI am really happy with the Anime Rating Prediction leaderboard. LLMs have so much potential as recommendation systems. They can understand connections way better than systems that just say \\\"this person is similar to you, here's what they like.\\\" They leaderboard also is in theory a great way to measure statistical analysis ability and just in general how much knowledge a model has. Though because of how miqu models and a few 8Bs are doing pretty well, it seems this is also a skill that some models are just better at.\\n\\nclaude-3-opus-20240229 is pretty impressive at giving ratings. It loses points because it mostly only rated models using 6,7,8 instead of the full range of 1-10, but of the three models it gave an 8, all three were in the top four highest rated anime of the test set. This can be seen by how high its correlation is. It's just a shame that its range of ratings is so small. If makes it harder to distinguish between the ratings it gives.\\n\\nUnfortunately with something like giving ratings predictions, it's really easy for models to choose slightly different numbers during different generations, even at temp 0. You really have to do all you can to make it as deterministic as possible.\\n\\n-----\\n\\nI also changed the leaderboard's font from HuggingFace's default to Segoe UI. Should be easier to read now.\\n\\nI'd love to hear your thoughts on the update and new leaderboards!\"}, {\"title\": \"Comprehensive benchmark of GGUF vs EXL2 performance across multiple models and sizes\", \"mode\": \"Hot\", \"upvotes\": 3, \"comments\": 0, \"url\": \"https://www.reddit.com/r/LocalLLaMA/comments/1e68k4o/comprehensive_benchmark_of_gguf_vs_exl2/\", \"content\": \"Hi!\\n\\nI've been wanting to test exl2 vs gguf for some time as it seems that the common consensus is that if you can fit the model into vram=use exl2 and if not=use gguf. But due to some models not being supported on exl2 I've been using gguf more lately, and noticing really good speeds.\\n\\nSo I did a whole set of tests at different model sizes to confirm what is the current state of exl2 and gguf. I tested llama3 8B, 70B and a bigger MoE like WizardLM2 8x22B to cover a wide variety of sizes.\\n\\nSystem:\\n\\nEpyc 7402\\n\\n512GB Ram at 3200MHz\\n\\n4x3090 at 250w cap\\n\\nLlama.cpp commit: [https://github.com/ggerganov/llama.cpp/commit/3807c3de04cde853418033c95e96642876545f3e](https://github.com/ggerganov/llama.cpp/commit/3807c3de04cde853418033c95e96642876545f3e)\\n\\nExllamav2 0.1.7 [https://github.com/turboderp/exllamav2](https://github.com/turboderp/exllamav2)\\n\\nTabbyapi commt [https://github.com/theroyallab/tabbyAPI/commit/e20a2d504b95b12560cb3a90d4841a7e9d6b0e1e](https://github.com/theroyallab/tabbyAPI/commit/e20a2d504b95b12560cb3a90d4841a7e9d6b0e1e)\\n\\nAll models quantized by me.\\n\\nAll test done with:\\n\\n606 Token context\\n\\n500 Token generation\\n\\nPrompt processing without caching Generation speed average though 3 runs\\n\\nGGUF: Tested with Flash attention enabled and Q4 cache too.\\n\\nEXL2: It's mandatory to use Flash attention as far as I know, also Q4 cache.\\n\\n|Model|Format|Quant|Prompt t/s|Generation t/s|Notes|Observations|\\n|:-|:-|:-|:-|:-|:-|:-|\\n|Llama 3 8B|GGUF|Q6\\\\_K|3899.16|92.22|\\\\~/llama.cpp/llama-server -m \\\\~/models/Meta-Llama-3-8B-Instruct-Q6\\\\_K.gguf -ngl 99 --host [0.0.0.0](http://0.0.0.0) --port 5000 -fa -ctk q4\\\\_0 -ctv q4\\\\_0 Llama.cpp splits the models across the 4xGPUs by default. Tested with CUDA\\\\_VISIBLE\\\\_DEVICES=0 but the speed was lower when using a single GPU.|Q6\\\\_K is equivalent to 6.56bpw|\\n|Llama 3 8B|EXL2|6.0bpw|3154.78|94.71|cache\\\\_mode: Q4, Rest of the settings as default so \\\"autosplit\\\" is enable but it only loads in a single GPU if it fits.||\\n|Llama 3 70B|GGUF|Q6\\\\_K|452.73|13.29|\\\\~/llama.cpp/llama-server -m \\\\~/models/Meta-Llama-3-70B-Instruct.Q6\\\\_K.gguf -ngl 99 --host [0.0.0.0](http://0.0.0.0) --port 5000 -fa -ctk q4\\\\_0 -ctv q4\\\\_0 It splits the model across of 4 gpus and it took 14/24GB of each 3090|Q6\\\\_K is equivalent to 6.56bpw|\\n|Llama 3 70B|EXL2|6.0bpw|442.61|14.36|cache\\\\_mode: Q4, Rest of the settings as default. It took 2 full gpu's + 1 half||\\n|WizardLM2 8x22B|GGUF|Q4\\\\_K\\\\_M|545.78|25.27|\\\\~/llama.cpp/llama-server -m \\\\~/models/WizardLM-2-8x22B-Q4\\\\_K\\\\_M.gguf -ngl 99 --host [0.0.0.0](http://0.0.0.0) --port 5000 -fa -ctk q4\\\\_0 -ctv q4\\\\_0 -c 32000|Q4\\\\_K\\\\_M is equivalent to 4.87bpw 32K context|\\n|WizardLM2 8x22B|EXL2|4.0bpw|315.16|24.53|cache\\\\_mode: Q4, Rest of the settings as default. Context 32K||\\n\\nConclusions: It seem like exl2 is a bit faster for llama3 8B (3% faster) and 70B (7% faster). But llama.cpp is faster in WizardLM2 8x22B by 3%\\n\\nLlama.cpp seems to have more development and contributors so it gets supports for new models faster. It's also more compatible with different platforms and allows for RAM offloading if the model doesn't fit in VRAM.\\n\\nIn general you cannot go wrong using exl2 in terms of performance, but you are not leaving much in the table if using gguf.\\n\\nNote: I'm not sure if the 6.0bpw and 4.0bpw in exl2 are exactly that size, llama.cpp server outputs the exact equivalent though. So it's not an exact comparison as each method of quantization yields different sizes event when using the \\\"same\\\" bits.\"}, {\"title\": \"China deploys censors to create socialist AI\", \"mode\": \"Hot\", \"upvotes\": 107, \"comments\": 149, \"url\": \"https://www.ft.com/content/10975044-f194-4513-857b-e17491d2a9e9\", \"content\": \"> The filtering begins with weeding out problematic information from training data and building a database of sensitive keywords. China\\u2019s operational guidance to AI companies published in February says AI groups need to collect thousands of sensitive keywords and questions that violate \\u201ccore socialist values\\u201d, such as \\u201cinciting the subversion of state power\\u201d or \\u201cundermining national unity\\u201d. The sensitive keywords are supposed to be updated weekly.\\n> But Chinese officials are also keen to avoid creating AI that dodges all political topics. The CAC has introduced limits on the number of questions LLMs can decline during the safety tests, according to staff at groups that help tech companies navigate the process. The quasi-national standards unveiled in February say LLMs should not reject more than 5 per cent of the questions put to them.\"}, {\"title\": \"GPT-4o in your webcam\", \"mode\": \"Hot\", \"upvotes\": 223, \"comments\": 41, \"url\": \"https://v.redd.it/bt1agl71u6dd1\", \"content\": \"\"}, {\"title\": \"Sam Altman says $27 million San Francisco mansion is a complete and utter \\u2018lemon\\u2019\", \"mode\": \"Hot\", \"upvotes\": 246, \"comments\": 156, \"url\": \"https://www.forbes.com.au/news/billionaires/sam-altman-says-27-million-mansion-is-a-lemon/\", \"content\": \"\"}, {\"title\": \"My friend made an AI generated music video for my ogre album, did he nail it?\", \"mode\": \"Hot\", \"upvotes\": 292, \"comments\": 71, \"url\": \"https://v.redd.it/qkpo1tmpq3dd1\", \"content\": \"\"}, {\"title\": \"Stuart Russell says AIs will develop a self-preservation instinct by default, because if you ask a robot to fetch a coffee, it will need to survive to achieve its goal\", \"mode\": \"Hot\", \"upvotes\": 15, \"comments\": 10, \"url\": \"https://v.redd.it/69a9rzajr7dd1\", \"content\": \"\"}, {\"title\": \"What is the likelihood that the average person will see a seismic shift to their lifestyle by 2030? \", \"mode\": \"Hot\", \"upvotes\": 11, \"comments\": 28, \"url\": \"https://www.reddit.com/r/OpenAI/comments/1e64doh/what_is_the_likelihood_that_the_average_person/\", \"content\": \"And what would you say are the chances that despite all the investment AI progress does could still hit an insurmountable wall and ultimately never reach any form of general intelligence (which is still very useful but not in a way that will revolutionize our lives or solve problems besetting our species or anything that we're hoping for)?\\n\\nI oscillate between thinking that everything will have changed by 2030 and nothing much will have changed. \\n\\nBut I think ultimately with all the brain power and funding being funneled into AI, I would be incredibly surprised and disappointed if we did hit a ceiling that we couldn't break through. I guess time will tell \"}, {\"title\": \"In the coming weeks\", \"mode\": \"Hot\", \"upvotes\": 40, \"comments\": 8, \"url\": \"https://i.redd.it/t1jw1vidy5dd1.jpeg\", \"content\": \"\"}, {\"title\": \"Prover-Verifier Games improve legibility of language model outputs\", \"mode\": \"Hot\", \"upvotes\": 5, \"comments\": 0, \"url\": \"https://openai.com/index/prover-verifier-games-improve-legibility/\", \"content\": \"\"}, {\"title\": \"So many people just can't imagine tech improving\", \"mode\": \"Hot\", \"upvotes\": 272, \"comments\": 62, \"url\": \"https://i.redd.it/87npgh6yn0dd1.png\", \"content\": \"\"}, {\"title\": \"How to create dataset. \", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 0, \"url\": \"https://www.reddit.com/r/OpenAI/comments/1e68r48/how_to_create_dataset/\", \"content\": \"I have been struggling to create a dataset in OpenAI's fine-tuning format. I have a document spanning nearly 235 pages that I want to convert into JSONL format. Can anyone help me with that?\"}, {\"title\": \"How far are we from autonomous software implementation?\", \"mode\": \"Hot\", \"upvotes\": 1, \"comments\": 3, \"url\": \"https://www.reddit.com/r/OpenAI/comments/1e67rce/how_far_are_we_from_autonomous_software/\", \"content\": \"Was talking with some startups recently, and with their currently developments in terms of multi AI agents performing simultaneous tasks, I wonder how long will it be for software implementations (like salesforce for instance) to be as complex as a simple prompt,\\n\\nWhat do you think?\"}, {\"title\": \"[R] Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?\", \"mode\": \"Hot\", \"upvotes\": 20, \"comments\": 1, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e5qt1r/r_spider2v_how_far_are_multimodal_agents_from/\", \"content\": \"A new benchmark for multimodal AI agents, focused on real-world Dara Engineering tasks.\\n\\nProject page: [link](https://spider2-v.github.io/), paper: [link](https://spider2-v.github.io/static/data/Spider2-V.pdf), code: [link](https://github.com/xlang-ai/Spider2-V).\\n\\n=====\\n\\nTLDR: Autonomous LLM-agents can\\u2019t replace Data Engineers\\u2026yet. But at least we can track progress \\ud83e\\udee1\\n\\nOverview:\\n\\nAs AI technologies become more advanced, we need increasingly complex benchmarks to evaluate the quality of systems and measure progress. A distinct branch of benchmarks has emerged, focusing on working with professional tools/applications and websites (see [WorkArena](https://github.com/ServiceNow/WorkArena), [WebArena](https://webarena.dev/), [OSWorld](https://os-world.github.io/)).\\n\\nIn the Spider2-V project, a benchmark is being created to evaluate AI agents in data engineering. It consists of 494 tasks covering the entire work cycle:\\n\\n* Data Warehousing (tools like Snowflake, BigQuery)\\n* Data Ingestion (e.g., Airbyte)\\n* Data Transformation (e.g., dbt)\\n* Data Visualization (e.g., Superset, Metabase)\\n* Data Orchestration (e.g., Airflow, Dagster)\\n\\n(and beloved Excel files, because who can do without them?)\\n\\nIf you have experience with data engineering, you understand that this is a substantial set, though it doesn't cover the entire zoo of solutions you might encounter.\\n\\n**Preparing each task took an average of 4 hours, so they are quite atomic and do not require very long horizon thinking**. Tasks are divided into three levels of difficulty:\\n\\n* Easy (20%, no more than 5 steps to solve)\\n* Medium (63%, 6-15 steps)\\n* Hard (17%, 16-40 steps)\\n\\nAll tasks are based on DE/DS tutorials, derived from the web by human labelers. Feasible to say that they represent real use cases. An example of a simple task:\\n\\n>Load data under the current Google Drive folder into a new table \\u201cdata1\\u201d of the opened BigQuery dataset\\n\\nOr a task of medium difficulty:\\n\\n>Install dbt-cloud-cli from GitHub and extract the binary to the same folder as the dbt project \\\"analytics\\u201d\\n\\nTo solve tasks, LLM agents have access to an IDE and a browser (with accounts set up). The model generates Python code using [pyautogui](https://github.com/asweigart/pyautogui) to interact with the UI of the virtual machine, then the code is executed, and the process repeats step by step.\\n\\n===\\n\\nGuess how many tasks GPT-4 completed? \\n\\nOnly 14%!  It seems low, but one can highlight more successful clusters\\u2014**40% of simple tasks and 25% of data visualization tasks were solved.**\\n\\nIn addition to proprietary models, open models (LLAMA 3 70B, Mixtral 8x7B) were tested, but since they are not multimodal and do not accept images as input, they were only shown a text description of the screen. This significantly lowered their metrics\\u2014they solved only a percentage of the tasks. However, we are eagerly awaiting LLAMA-3 405B, rumored to be multimodal and set to be released on July 23rd.\\n\\n===\\n\\nI am VERY eager to see the benchmark metric published with the release of GPT-5\\u2014then we'll see! >!Place your bets on what percentage of tasks the next-generation models will solve! !<\"}, {\"title\": \"[D] Strange behaviour with training on 3090\", \"mode\": \"Hot\", \"upvotes\": 2, \"comments\": 0, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e650gq/d_strange_behaviour_with_training_on_3090/\", \"content\": \"I'm training a source separation model, or more specifically finetuning one. [From this repo ](https://github.com/ZFTurbo/Music-Source-Separation-Training)\\n\\nThe issue is this: the original checkpoint I'm trying to finetune has a SDR value of about 11. When I start finetuning (although with just batch size 1 compared to the original 16), after the first epoch is done my SDR value is 0.0016 and something like that. Then the next epoch it's about 3, then the next is 6, then it just more gradually goes up to 6-7.\\n\\nThis shouldn't be happening - the SDR should be close to the original value (11) right from the first epoch. \\n\\nWhy do I think so? The repo also allows you to validate the checkpoints for SDR without training - and when I validate that 0.0016 SDR checkpoint again, it tells me the SDR is actually about 11, like it should be. But during training, it's much smaller.\\n\\nThe author told me it could be Pytorch issue but even on the latest version, the issue persist. For what it's worth, I've done the exact same training but on cloud A6000/A100/H100 and the issue is not there. The SDR values are completely normal from the first epoch.\\n\\nIs this just the 3090 not being enough or there is a bug somewhere? All other loss values are also within the normal range. \"}, {\"title\": \"[N] Tom's Hardware reviews Gigabyte local AI training product AI TOP\", \"mode\": \"Hot\", \"upvotes\": 0, \"comments\": 3, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e61utc/n_toms_hardware_reviews_gigabyte_local_ai/\", \"content\": \"Looks pretty nifty, might be helpful to the ML community. Review here: https://www.tomshardware.com/tech-industry/artificial-intelligence/gigabyte-releases-ai-software-to-help-train-your-own-ai\\n\\nProduct page here: https://www.gigabyte.com/WebPage/1079?lan=en\"}, {\"title\": \"[D] Author of ReFT: Representation Finetuning for Language Models, at Oxen.ai Paper Club this Friday\", \"mode\": \"Hot\", \"upvotes\": 13, \"comments\": 3, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e5h1m8/d_author_of_reft_representation_finetuning_for/\", \"content\": \"Arxiv paper first author **Zhengxuan Wu** will join **Greg Schoeninger**\\u00a0in this Friday's  [Oxen.AI](http://Oxen.AI) Paper Club to explain how editing representations can be better than Parameter-efficient finetuning (PEFT) methods. [https://lu.ma/oxen](https://lu.ma/oxen)\\n\\n**ReFT**: Representation Finetuning for Language Models.\\n\\nGreg, 3 questions, 1 comment from reading just the abstract.\\n\\n**Q1)** What exactly is meant by a \\\"representation\\\"?\\n\\ni.e., what part of the neural network is captured by what this paper refers to as a representation?\\n\\n**Q2)** What is meant by an intervention in \\\"task specific intervention\\\" ?   I haven't heard that term before with pretraining or fine-tuning.\\n\\n**Q3)** In API terms, would the point of this paper be like saying:\\n\\nInstead of improving the API by improving the documented inputs and outputs, we will improve the API by directly changing the code?\\n\\n**Comment**) The abstract makes this paper sound very voodoo.   Hope testing was apples :: apples.\\n\\nLook forward to your demystification on Friday, Greg.  So cool that you have the paper's first author joining you to help explain and answer questions.\\n\\n**Deets**:\\n\\n[https://lu.ma/oxen](https://lu.ma/oxen)\\n\\nFriday July 19, 10:00 AM Pacific, 1:00 PM Eastern Time on Zoom\\n\\nPaper: [https://arxiv.org/pdf/2404.03592](https://arxiv.org/pdf/2404.03592)\\n\\nGratitude:   Thank you Greg, u/FallMindless3563, Scott Howard u/sthoward, and the Oxen team for giving me an Easy button and for sharing your knowledge with the community while providing cool tools to curate datasets at oxen.ai.\"}, {\"title\": \"[P] Matching segment areas in medical images\", \"mode\": \"Hot\", \"upvotes\": 8, \"comments\": 5, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e5i610/p_matching_segment_areas_in_medical_images/\", \"content\": \"https://preview.redd.it/2p5lksh1z2dd1.png?width=597&format=png&auto=webp&s=1995475c783500ab58e9564e140b8debdf7dc8f3\\n\\nReferring to the attached image, I am wrestling with problem of building a deep learning network capable of finding which segmented area in the left image is the body section matching area 1 in the right (with the red number). Could anyone share pointers to where this challenge was addressed or in any case what is the name of the problem so that I can search for papers and code? Thanks in advance, I am open for working together also, this is for explainable AI in the context of heart disease. \"}, {\"title\": \"[D] About the dimensions of latents in stable diffusion\", \"mode\": \"Hot\", \"upvotes\": 3, \"comments\": 13, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e5qszw/d_about_the_dimensions_of_latents_in_stable/\", \"content\": \"Hello, I have been wondering about this for a while, hopefully you can shed some light on this doubt of mine.\\n\\nGiven that the autoencoder in latent (stable) diffusion is trained to produce perceptually similar latents with respect to input images, it seems odd the authors chose 4x64x64 as the dimensions for the latents; why add a channel?\\n\\nA choice of 3x64x64 would be much more justifiable since it could be argued that the autoencoder will learn to map each of the channels of the input image to a channel in the latent, thus preserving the perceptual similarity as much as possible in latent space.\\n\\nSo I guess the discussion topic is: Why was a latent with 4 channels chosen?\"}, {\"title\": \"What steps should be taken to understand up to Variational Auto-Encoders? [R]\", \"mode\": \"Hot\", \"upvotes\": 5, \"comments\": 6, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e5idrz/what_steps_should_be_taken_to_understand_up_to/\", \"content\": \"Starting up with ML now and need to use VAEs for a research group. \\n\\n\\n\\nWhat concepts should I understand before I can focus on their application?\"}, {\"title\": \"[D] Best places to rent GPUs from\", \"mode\": \"Hot\", \"upvotes\": 23, \"comments\": 19, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e562n1/d_best_places_to_rent_gpus_from/\", \"content\": \"Hello guys,\\n\\nI want to have the flexibility to rent GPUs on demand and ofc not pay a lot. I have been looking at couple companies like brev.Dev, runpod and fluidstack. I wanted to know if you guys are using any of these or something different to run your workloads \"}, {\"title\": \"How to Compare Sensitivity Between XGBoost and Expert Classification with Unclassified Observations? \\u201c[Research]\\u201d\", \"mode\": \"Hot\", \"upvotes\": 2, \"comments\": 2, \"url\": \"https://www.reddit.com/r/MachineLearning/comments/1e5jb5p/how_to_compare_sensitivity_between_xgboost_and/\", \"content\": \"I have a machine learning project involving multi-class classification with three classes: M, S, and U. My test dataset includes 236 observations. I used XGBoost and measured sensitivity as the evaluation metric.\\n\\nI want to compare the results with expert classification on the test dataset. However, the expert classified only 68 observations and left 168 observations unclassified.\\n\\nHow can I compare the sensitivity of XGBoost with the expert\\u2019s classification given this discrepancy in the number of classified observations?\\n\\nHere are the confusion matrices:\\n\\nXGBoost confusion matrix:\\n\\n[[ 17  25   4]\\n [ 19 120  15]\\n [  4  22  10]]\\n\\nExpert confusion matrix:\\n\\n[[ 7  3  2]\\n [ 5 13 30]\\n [ 0  0  8]]\"}]",
    "tools": [
        {
            "name": "generate_reddit_post_summary",
            "description": "Generate a summary of Reddit posts in a specific JSON format.",
            "input_schema": {
                "type": "object",
                "required": [
                    "posts"
                ],
                "properties": {
                    "posts": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "required": [
                                "title",
                                "url",
                                "main_topic",
                                "key_points",
                                "potential_impact",
                                "relevance"
                            ],
                            "properties": {
                                "url": {
                                    "type": "string",
                                    "description": "The URL of the Reddit post."
                                },
                                "title": {
                                    "type": "string",
                                    "description": "The title of the Reddit post."
                                },
                                "relevance": {
                                    "type": "string",
                                    "description": "Relevance of the post to current ML/GenAI trends."
                                },
                                "key_points": {
                                    "type": "array",
                                    "items": {
                                        "type": "string"
                                    },
                                    "description": "Key points discussed in the post."
                                },
                                "main_topic": {
                                    "type": "string",
                                    "description": "Main topic or technology discussed in the post."
                                },
                                "potential_impact": {
                                    "type": "string",
                                    "description": "Description of the post's potential impact."
                                }
                            }
                        },
                        "description": "Array of Reddit post summaries."
                    }
                }
            }
        }
    ]
}